{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_xuEtMEu-Vfs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xuEtMEu-Vfs",
    "outputId": "430fac90-f996-44f4-d0b5-df507e22cd35"
   },
   "outputs": [],
   "source": [
    "## For google collab\n",
    "\n",
    "#!pip install datasets evaluate transformers\n",
    "#!pip install -U datasets huggingface_hub fsspec\n",
    "#!pip install peft\n",
    "#!apt-get install cd-hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13892be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine the DIBS and MFIB sequences, and cluster them using CD-HIT\n",
    "\n",
    "#!cat DIBS.fasta mfib.fasta > do_transition.fasta \n",
    "#!cd-hit -i do_transition.fasta -o do_transition_cdhit.fasta -c 0.7\n",
    "#!cd-hit -i fuzdb.fasta -o dd_transition_cdhit.fasta -c 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1928067",
   "metadata": {
    "id": "d1928067"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbb003",
   "metadata": {
    "id": "c6bbb003"
   },
   "outputs": [],
   "source": [
    "# Read a FASTA file and return a list of sequences.\n",
    "\n",
    "def read_fasta(file_path):\n",
    "    sequences=[]\n",
    "    current_sequence=[]\n",
    "    with open(file_path,\"r\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if current_sequence:\n",
    "                    sequences.append(''''''.join(current_sequence))\n",
    "                    current_sequence=[]\n",
    "            else:\n",
    "                current_sequence.append(line.strip())\n",
    "        if current_sequence:\n",
    "            sequences.append(''''''.join(current_sequence))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29c59a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db29c59a",
    "outputId": "31ac22d6-fffc-4a05-c990-b7c6f4be29a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFIB+DIBS:\n",
      "count    614.000000\n",
      "mean      49.609121\n",
      "std       50.737725\n",
      "min       11.000000\n",
      "25%       16.000000\n",
      "50%       32.000000\n",
      "75%       70.000000\n",
      "max      630.000000\n",
      "Name: length, dtype: float64\n",
      "fuzdb:\n",
      "count     316.000000\n",
      "mean      559.186709\n",
      "std       594.045469\n",
      "min        30.000000\n",
      "25%       171.750000\n",
      "50%       407.000000\n",
      "75%       709.000000\n",
      "max      4953.000000\n",
      "Name: length, dtype: float64\n",
      "Number of removed sequences: 42\n",
      "Unique sequences count: 880\n"
     ]
    }
   ],
   "source": [
    "# Read the sequences from the FASTA files and create DataFrames for each dataset\n",
    "\n",
    "sequences_do = read_fasta(\"do_transition_cdhit.fasta\") # clustered MFIB+DIBS sequences by CD-HIT by 0.7\n",
    "sequences_dd = read_fasta(\"dd_transition_cdhit.fasta\") # clustered fuzdb sequences by CD-HIT by 0.7\n",
    "\n",
    "\n",
    "df_do = pd.DataFrame(sequences_do, columns=[\"sequence\"])\n",
    "df_do[\"label\"] = 0\n",
    "df_dd = pd.DataFrame(sequences_dd, columns=[\"sequence\"])\n",
    "df_dd[\"label\"] = 1\n",
    "\n",
    "\n",
    "# Calculate the length of each sequence and add it as a new column\n",
    "\n",
    "df_do[\"length\"] = df_do[\"sequence\"].apply(len)\n",
    "df_dd[\"length\"] = df_dd[\"sequence\"].apply(len)\n",
    "\n",
    "print(\"MFIB+DIBS:\",df_do[\"length\"].describe(), sep=\"\\n\")\n",
    "print(\"fuzdb:\",df_dd[\"length\"].describe(), sep=\"\\n\")\n",
    "\n",
    "# Filter out sequences longer than 1000 characters\n",
    "\n",
    "print(\"Number of removed sequences:\" ,df_dd[df_dd[\"length\"] > 1000][\"sequence\"].count())\n",
    "df_dd = df_dd[df_dd[\"length\"] < 1001]\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "\n",
    "df_all = pd.concat([df_do, df_dd], ignore_index=True)\n",
    "\n",
    "# Remove all sequences that are repeated more than once\n",
    "\n",
    "sequence_counts = df_all[\"sequence\"].value_counts()\n",
    "unique_sequences = sequence_counts[sequence_counts == 1].index\n",
    "df_unique = df_all[df_all[\"sequence\"].isin(unique_sequences)].reset_index(drop=True)\n",
    "print(\"Unique sequences count:\", df_unique.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0faa569",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0faa569",
    "outputId": "51d98e87-bb74-4587-86bd-ebff32fc7d71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 748\n",
      "Test set size: 132\n",
      "Train set label distribution:\n",
      " label\n",
      "0    518\n",
      "1    230\n",
      "Name: count, dtype: int64\n",
      "Test set label distribution:\n",
      " label\n",
      "0    92\n",
      "1    40\n",
      "Name: count, dtype: int64\n",
      "Train set length distribution:\n",
      " count    748.000000\n",
      "mean     155.312834\n",
      "std      213.010061\n",
      "min       11.000000\n",
      "25%       22.000000\n",
      "50%       64.000000\n",
      "75%      153.250000\n",
      "max      979.000000\n",
      "Name: length, dtype: float64\n",
      "Test set length distribution:\n",
      " count    132.000000\n",
      "mean     133.265152\n",
      "std      196.219415\n",
      "min       11.000000\n",
      "25%       18.000000\n",
      "50%       54.500000\n",
      "75%      136.750000\n",
      "max      917.000000\n",
      "Name: length, dtype: float64\n",
      "Columns in train set and test set: ['sequence', 'label', 'length'] ['sequence', 'label', 'length']\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets, stratifying by label\n",
    "\n",
    "train_df, test_df = train_test_split(df_unique, test_size=0.15, stratify=df_unique[\"label\"], random_state=42, shuffle=True)\n",
    "\n",
    "print(\"Train set size:\", train_df.shape[0])\n",
    "print(\"Test set size:\", test_df.shape[0])\n",
    "print(\"Train set label distribution:\n\", train_df[\"label\"].value_counts())\n",
    "print(\"Test set label distribution:\n\", test_df[\"label\"].value_counts())\n",
    "print(\"Train set length distribution:\n\", train_df[\"length\"].describe())\n",
    "print(\"Test set length distribution:\n\", test_df[\"length\"].describe())\n",
    "print(\"Columns in train set and test set:\", train_df.columns.tolist(), test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ffe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer for the ESM-2 model\n",
    "\n",
    "checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Function to tokenize the sequences in the dataset\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sequence\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7540d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrames to Hugging Face Datasets and tokenize them\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Train datset:\", train_dataset)\n",
    "print(\"Test datset:\", test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d1222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns from the datasets\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"sequence\", \"length\"])\n",
    "test_dataset = test_dataset.remove_columns([\"sequence\", \"length\"])\n",
    "\n",
    "print(\"Train datset:\", train_dataset)\n",
    "print(\"Test datset:\", test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded4003",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Class Wighting to handle class imbalance\n",
    "\n",
    "# Calculate Weights\n",
    "\n",
    "label_counts = train_dataset.to_pandas()['label'].value_counts()\n",
    "print(f\"Label counts: {label_counts}\")\n",
    "num_samples_class_0 = label_counts[0]\n",
    "num_samples_class_1 = label_counts[1]\n",
    "total_samples = len(train_dataset)\n",
    "\n",
    "weight_for_0 = total_samples / (2 * num_samples_class_0)\n",
    "weight_for_1 = total_samples / (2 * num_samples_class_1)\n",
    "\n",
    "class_weights = torch.tensor([weight_for_0, weight_for_1])\n",
    "\n",
    "# Move to GPU if available and convert to float32\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "class_weights = class_weights.to(device).to(torch.float32)\n",
    "\n",
    "print(f\"Calculated class weights: {class_weights}\")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Step 2: Create a Custom Trainer\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=batch_size, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Define the loss function with your calculated weights\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lora-config-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model for sequence classification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Set up the training arguments\n",
    "\n",
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "batch_size = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    model_name,\n",
    "    warmup_steps=0,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    report_to = \"none\",\n",
    ")\n",
    "\n",
    "\n",
    "# Load the metrics\n",
    "\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "# Function to compute metrics during evaluation\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    results = {}\n",
    "    # Use 'weighted' average to account for imbalance in the report\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\"))\n",
    "    results.update(precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\"))\n",
    "    results.update(recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Create a Trainer instance with the model, training arguments, datasets, tokenizer, and metrics function\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "le8CJqoMG49r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the Trainer instance\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yGti0cEIIXYq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "\n",
    "trainer.save_model(\"my_model_dir\")\n",
    "tokenizer.save_pretrained(\"my_model_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be05c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the log history from the trainer state\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "df_log = pd.DataFrame(log_history)\n",
    "\n",
    "# Save the log history to a CSV file\n",
    "df_log.to_csv(\"training_logs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}