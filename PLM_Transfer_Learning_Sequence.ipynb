{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_xuEtMEu-Vfs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xuEtMEu-Vfs",
    "outputId": "8c10f8f9-7736-407e-abf2-ba27e9e4a140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.5\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.7)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers\n",
    "!pip install -U datasets huggingface_hub fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1928067",
   "metadata": {
    "id": "d1928067"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbb003",
   "metadata": {
    "id": "c6bbb003"
   },
   "outputs": [],
   "source": [
    "# Read a FASTA file and return a list of sequences.\n",
    "\n",
    "def read_fasta(file_path):\n",
    "    sequences=[]\n",
    "    current_sequence=[]\n",
    "    with open(file_path,\"r\") as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if current_sequence:\n",
    "                    sequences.append(''.join(current_sequence))\n",
    "                    current_sequence=[]\n",
    "            else:\n",
    "                current_sequence.append(line.strip())\n",
    "        if current_sequence:\n",
    "            sequences.append(''.join(current_sequence))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29c59a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db29c59a",
    "outputId": "b14b2d84-1ec5-4f05-b908-1a5a478e1329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFIB+DIBS:\n",
      "count    614.000000\n",
      "mean      49.609121\n",
      "std       50.737725\n",
      "min       11.000000\n",
      "25%       16.000000\n",
      "50%       32.000000\n",
      "75%       70.000000\n",
      "max      630.000000\n",
      "Name: length, dtype: float64\n",
      "fuzdb:\n",
      "count     316.000000\n",
      "mean      559.186709\n",
      "std       594.045469\n",
      "min        30.000000\n",
      "25%       171.750000\n",
      "50%       407.000000\n",
      "75%       709.000000\n",
      "max      4953.000000\n",
      "Name: length, dtype: float64\n",
      "Number of removed sequences: 42\n",
      "Unique sequences count: 880\n"
     ]
    }
   ],
   "source": [
    "# Read the sequences from the FASTA files and create DataFrames for each dataset\n",
    "\n",
    "sequences_do = read_fasta(\"clustered_fasta_files/do_transition_cdhit.fasta\") # clustered MFIB+DIBS sequences by CD-HIT by 0.7\n",
    "sequences_dd = read_fasta(\"clustered_fasta_files/dd_transition_cdhit.fasta\") # clustered fuzdb sequences by CD-HIT by 0.7\n",
    "\n",
    "\n",
    "df_do = pd.DataFrame(sequences_do, columns=[\"sequence\"])\n",
    "df_do[\"label\"] = 0\n",
    "df_dd = pd.DataFrame(sequences_dd, columns=[\"sequence\"])\n",
    "df_dd[\"label\"] = 1\n",
    "\n",
    "\n",
    "# Calculate the length of each sequence and add it as a new column\n",
    "\n",
    "df_do[\"length\"] = df_do[\"sequence\"].apply(len)\n",
    "df_dd[\"length\"] = df_dd[\"sequence\"].apply(len)\n",
    "\n",
    "print(\"MFIB+DIBS:\",df_do[\"length\"].describe(), sep=\"\\n\")\n",
    "print(\"fuzdb:\",df_dd[\"length\"].describe(), sep=\"\\n\")\n",
    "\n",
    "# Filter out sequences longer than 1000 characters\n",
    "\n",
    "print(\"Number of removed sequences:\" ,df_dd[df_dd[\"length\"] > 1000][\"sequence\"].count())\n",
    "df_dd = df_dd[df_dd[\"length\"] < 1001]\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "\n",
    "df_all = pd.concat([df_do, df_dd], ignore_index=True)\n",
    "\n",
    "# Remove all sequences that are repeated more than once\n",
    "\n",
    "sequence_counts = df_all[\"sequence\"].value_counts()\n",
    "unique_sequences = sequence_counts[sequence_counts == 1].index\n",
    "df_unique = df_all[df_all[\"sequence\"].isin(unique_sequences)].reset_index(drop=True)\n",
    "print(\"Unique sequences count:\", df_unique.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0faa569",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0faa569",
    "outputId": "ed7c9f9b-e154-4f99-abc9-4075d872ae73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 748\n",
      "Test set size: 132\n",
      "Train set label distribution:\n",
      " label\n",
      "0    518\n",
      "1    230\n",
      "Name: count, dtype: int64\n",
      "Test set label distribution:\n",
      " label\n",
      "0    92\n",
      "1    40\n",
      "Name: count, dtype: int64\n",
      "Train set length distribution:\n",
      " count    748.000000\n",
      "mean     155.312834\n",
      "std      213.010061\n",
      "min       11.000000\n",
      "25%       22.000000\n",
      "50%       64.000000\n",
      "75%      153.250000\n",
      "max      979.000000\n",
      "Name: length, dtype: float64\n",
      "Test set length distribution:\n",
      " count    132.000000\n",
      "mean     133.265152\n",
      "std      196.219415\n",
      "min       11.000000\n",
      "25%       18.000000\n",
      "50%       54.500000\n",
      "75%      136.750000\n",
      "max      917.000000\n",
      "Name: length, dtype: float64\n",
      "Columns in train set and test set: ['sequence', 'label', 'length'] ['sequence', 'label', 'length']\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets, stratifying by label\n",
    "\n",
    "train_df, test_df = train_test_split(df_unique, test_size=0.15, stratify=df_unique[\"label\"], random_state=42, shuffle=True)\n",
    "\n",
    "print(\"Train set size:\", train_df.shape[0])\n",
    "print(\"Test set size:\", test_df.shape[0])\n",
    "print(\"Train set label distribution:\\n\", train_df[\"label\"].value_counts())\n",
    "print(\"Test set label distribution:\\n\", test_df[\"label\"].value_counts())\n",
    "print(\"Train set length distribution:\\n\", train_df[\"length\"].describe())\n",
    "print(\"Test set length distribution:\\n\", test_df[\"length\"].describe())\n",
    "print(\"Columns in train set and test set:\", train_df.columns.tolist(), test_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ffe07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "dd7a672b085c4c02923a16de296d9c94",
      "3303ec41b4274eb9a7e5c8b079490337",
      "171d42d3fe6145edb28686dea7595410",
      "aa164ea3cc3b43e78e6465772854ec1a",
      "90c7041966844bdd9a1b5e1cd29cb8fb",
      "dcd679cb8ba64a3b9e300706136d9f2a",
      "7f41dace8873430bb114fa7a609f8254",
      "54d9f1875e4e44ef8937990541dd53ec",
      "d3d99ccce43a4110943c0a3aeea836a6",
      "c08f3eebda18474eb9abb1352847b1cd",
      "8275e00650e44b9394bbee7f7d4fb8f2",
      "cf94cdd555a340c3aefc0baf2d5d568e",
      "539b6b3e6b974f3996ed5fda4ef4b49c",
      "b9705baaac55480a8cdb2901a883ff15",
      "ab67e00feadb4c719e82d68930cc3f03",
      "8b714211a4b84314afdf8dbb5abae4bb",
      "b204d255011f4570859e71d5394ee314",
      "3d9403427000450082bfacee805eef4e",
      "ce242a0f420d4afcb536b3b182108c89",
      "b6caa9d9588749fe99b5758d295317e8",
      "415dd179086048a99a5e6762519aa8c1",
      "29d7339977a449cc840f9f5aa1a93e0a",
      "628c352896704df18ed34e33d9c1ae9a",
      "19895065b2a1413fa5ec2ae2e2eaf1e4",
      "4a07685fadba4bed9f2d610570aeb916",
      "81469a64df8444d39097c33fc00c0170",
      "a3dc9c5861eb4ac8badf78fbb41f99d3",
      "7d2cd11c02a740be9aa61f6182b2a139",
      "cf49f3a834f74107b3848a1d644be261",
      "36df544438af4844aadfe3be292cdc2c",
      "9c6917cdcb784cf0b29293838a2ac316",
      "72bb2d606cb249b98112b8e802fae47b",
      "42c25a0d17084cb1ab77ca79ed052206"
     ]
    },
    "id": "657ffe07",
    "outputId": "e98feb8e-37d6-45ac-97c9-556497cdb02e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7a672b085c4c02923a16de296d9c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf94cdd555a340c3aefc0baf2d5d568e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628c352896704df18ed34e33d9c1ae9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the tokenizer for the ESM-2 model\n",
    "\n",
    "checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Function to tokenize the sequences in the dataset\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sequence\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7540d4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "9e864dd173e64a36942aad88b3efd729",
      "b35c2276058e4cd6a6c83dc9427cac3e",
      "f4c5e6b2b1814aa68cbb60fbb204fd33",
      "107008e681e84aecbb15efeba323cda3",
      "8f02b241fa1d4c5e992b6cb432e12195",
      "2b25f756a0bd4c4aa06b1b5ddb84cb44",
      "ec5aa1c0a220427eafe6a9f9e4456abe",
      "a3ae0d0c8cb04a09a460512f454c9618",
      "91c10e057b7a4a21b76a33ed51ef5257",
      "801cfa313e274eacbdd9665bc0bc26e1",
      "9a7ae31e17b14b749f254d72935e1691",
      "23debfc2c52b4e1a8b5bb4aca461d8f1",
      "6e61ca4da1e04b6c82dbe2bd3e07dafe",
      "e04fdf7617a24f10bd27e0770e1da399",
      "009e4afdf32b490c839d04e46cf821aa",
      "becc0b8a5f7f4046bec606e476af0fc1",
      "a95c89ab0845461c8beedd17009d916a",
      "0d09179578464266bb82df79df4afc1e",
      "36d9a313e17344e88438e2f04e421f5c",
      "6b9de3213f784bf1ac2bcc62e7da13a4",
      "94b70261e5624c53a434df2a6c72bcc8",
      "196c3c4c1ffb4587aa58e7507adf94b7"
     ]
    },
    "id": "b7540d4a",
    "outputId": "9be6acee-02a6-4ee3-8c89-129a94aefe34"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e864dd173e64a36942aad88b3efd729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23debfc2c52b4e1a8b5bb4aca461d8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datset: Dataset({\n",
      "    features: ['sequence', 'label', 'length', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 748\n",
      "})\n",
      "Test datset: Dataset({\n",
      "    features: ['sequence', 'label', 'length', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 132\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrames to Hugging Face Datasets and tokenize them\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Train datset:\", train_dataset)\n",
    "print(\"Test datset:\", test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d1222",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5d1222",
    "outputId": "838ffabb-e355-4ce0-8e34-c50e20965f59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datset: Dataset({\n",
      "    features: ['label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 748\n",
      "})\n",
      "Test datset: Dataset({\n",
      "    features: ['label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 132\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns from the datasets\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"sequence\", \"length\"])\n",
    "test_dataset = test_dataset.remove_columns([\"sequence\", \"length\"])\n",
    "\n",
    "print(\"Train datset:\", train_dataset)\n",
    "print(\"Test datset:\", test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded4003",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bded4003",
    "outputId": "0c29d585-75ed-4cc6-d84e-be19218bb4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts: label\n",
      "0    518\n",
      "1    230\n",
      "Name: count, dtype: int64\n",
      "Calculated class weights: tensor([0.7220, 1.6261], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "##  Class Wighting to handle class imbalance\n",
    "\n",
    "# Calculate Weights\n",
    "\n",
    "label_counts = train_dataset.to_pandas()['label'].value_counts()\n",
    "print(f\"Label counts: {label_counts}\")\n",
    "num_samples_class_0 = label_counts[0]\n",
    "num_samples_class_1 = label_counts[1]\n",
    "total_samples = len(train_dataset)\n",
    "\n",
    "weight_for_0 = total_samples / (2 * num_samples_class_0)\n",
    "weight_for_1 = total_samples / (2 * num_samples_class_1)\n",
    "\n",
    "class_weights = torch.tensor([weight_for_0, weight_for_1])\n",
    "\n",
    "# Move to GPU if available and convert to float32\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "class_weights = class_weights.to(device).to(torch.float32)\n",
    "\n",
    "print(f\"Calculated class weights: {class_weights}\")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Step 2: Create a Custom Trainer\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=batch_size, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Define the loss function with your calculated weights\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5df8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232,
     "referenced_widgets": [
      "56769903a4a94397a97077f9b343ada1",
      "c03515b495cb46e685c801373abea96e",
      "2ade916e9558480b92bb20cd83c25a88",
      "5a966b9045d44cd2b69479ed43cb6f02",
      "f6f1e64d7a5d48c29c883bf2910ba647",
      "06354fef9cce4b67a6ef1064155d4a53",
      "dbf9d5a4a8ff41dc9847f8452d15e584",
      "97ba9692737c4f909df774143ed37be7",
      "45e467dc37454e64a031c0748e2df1aa",
      "f266dac57cb54fe7a98e9371f9958afe",
      "f19cba635089466d96e8213d50a386b4",
      "d2c71fad87304ab7a94ff1b353d5fa2c",
      "46c906b977684eaabd98df3d29ca52ba",
      "dcb0deec4c06472491d11ce61aa91e61",
      "877cad1f35804d66b4a55543f437cd0a",
      "3e22fc2dec7e476fb89570fe9521853b",
      "cdeef63d1006460da0a610937bf2e930",
      "510d88bf79fb4698a56a6f0be2c8b21f",
      "159ac3a688c4420fa9c5f1839e8c4e4a",
      "b21caa4ccfad42a4a1064f50e0e7a3cd",
      "f4df6a9f78b44429a893f1a4d24a5b27",
      "2453fd50bab642ae8c29e688d0ae1eab",
      "d55bdbdbbda44b1da049e02ff3aa508c",
      "62f3e87fe8aa4095ae1ad669d10ee90d",
      "6c662396989f45bdbbb518dd89803422",
      "f772780f5648493181ea72f1009c9a6d",
      "a09593e7e7fc4f959c0282d1fbcecf70",
      "ff264e39dfbb4c6eb76d68790cfaaefa",
      "0cba0a04c2d740a2bd24a9dafa7ae78c",
      "21b195fdf19f47a3b06dac9d7b9ee2a2",
      "e048f3f4b00d45639fa787c94349e2b6",
      "f3be556ec3dc459cb83285b3509baa9f",
      "92c4e80ef4764107814ef6c5066bbac0",
      "e9a8d7b97566414d803a3cbd02d88935",
      "d732321e210246a0a03f8d36cac521ed",
      "f8cf265e49bb415dba6d697f9f72a96e",
      "1d12cfbdea6b4a389d6699339e8681e1",
      "926811e419f74a8d9011812db644fd62",
      "2a08153425494ca3ae2ca4d7ad3302dd",
      "3edceedfc42543a9bef17f6b0604b2f9",
      "eae6893c1dd749f98587149c40fb6dbe",
      "07f46741ea6f4a42aa61a9cdf5ed2ecc",
      "a738c7dd7ef643df9e142c3e2823567e",
      "22f4d3710d014fde94856914e79438dd",
      "f0c34b36d59144e5b08d8c9edfe2b10d",
      "a50a10dedb5f4d7999525930804b109d",
      "0a128774b81244f681b1d6d206549ecc",
      "58266e294dd74c49b4dfeb330a7f977f",
      "1b21e189a97940698a6c73927d654aa1",
      "581b73fad21e44aba31c7519f1d116a6",
      "6f94e469d7e74edea5cb05e972f50898",
      "e046edc3a7e94b20a18a899b33efe4d9",
      "10020caee574498e93c97fdfc2ad2b4a",
      "a0ff247abdf147b2a2d83d947d8422be",
      "c93f71d786ec4576bdbe7469b278ff60"
     ]
    },
    "id": "4ad5df8a",
    "outputId": "739136a5-1b3a-45a0-c870-7eaf2fd1d09e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56769903a4a94397a97077f9b343ada1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/778 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c71fad87304ab7a94ff1b353d5fa2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/136M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55bdbdbbda44b1da049e02ff3aa508c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a8d7b97566414d803a3cbd02d88935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c34b36d59144e5b08d8c9edfe2b10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pre-trained model for sequence classification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# Set up the training arguments\n",
    "\n",
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "batch_size = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    model_name,\n",
    "    warmup_steps=0,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    report_to = \"none\",\n",
    ")\n",
    "\n",
    "\n",
    "# Load the metrics\n",
    "\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "# Function to compute metrics during evaluation\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    results = {}\n",
    "    # Use 'weighted' average to account for imbalance in the report\n",
    "    results.update(f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\"))\n",
    "    results.update(precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\"))\n",
    "    results.update(recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Create a Trainer instance with the model, training arguments, datasets, tokenizer, and metrics function\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "le8CJqoMG49r",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "le8CJqoMG49r",
    "outputId": "2b14fe6c-c00a-4c97-f97d-222f5433329d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='935' max='935' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [935/935 03:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.183998</td>\n",
       "      <td>0.931571</td>\n",
       "      <td>0.931448</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.145400</td>\n",
       "      <td>0.209407</td>\n",
       "      <td>0.938444</td>\n",
       "      <td>0.939604</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.196835</td>\n",
       "      <td>0.946362</td>\n",
       "      <td>0.946937</td>\n",
       "      <td>0.946970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.234371</td>\n",
       "      <td>0.938444</td>\n",
       "      <td>0.939604</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.283874</td>\n",
       "      <td>0.938444</td>\n",
       "      <td>0.939604</td>\n",
       "      <td>0.939394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=935, training_loss=0.12861075681798598, metrics={'train_runtime': 236.6519, 'train_samples_per_second': 15.804, 'train_steps_per_second': 3.951, 'total_flos': 292904834669784.0, 'train_loss': 0.12861075681798598, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the Trainer instance\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "yGti0cEIIXYq",
   "metadata": {
    "id": "yGti0cEIIXYq"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_pretrained(\"PLM_Transfer_Sequence_Outputs/my_model_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(\"PLM_Transfer_Sequence_Outputs/tokenizer_dir\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_VqCV3JLRcm",
    "outputId": "4dbe5c2a-a11d-4ddc-8cd1-797d511895ff"
   },
   "id": "q_VqCV3JLRcm",
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('PLM_Transfer_Sequence_Outputs/tokenizer_dir/tokenizer_config.json',\n",
       " 'PLM_Transfer_Sequence_Outputs/tokenizer_dir/special_tokens_map.json',\n",
       " 'PLM_Transfer_Sequence_Outputs/tokenizer_dir/vocab.txt',\n",
       " 'PLM_Transfer_Sequence_Outputs/tokenizer_dir/added_tokens.json')"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0be05c44",
   "metadata": {
    "id": "0be05c44"
   },
   "outputs": [],
   "source": [
    "# Retrieve the log history from the trainer state\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "df_log = pd.DataFrame(log_history)\n",
    "\n",
    "# Save the log history to a CSV file\n",
    "df_log.to_csv(\"PLM_Transfer_Sequence_Outputs/training_logs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}